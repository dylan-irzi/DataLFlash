{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DataLFlash Demo: 3 Steps to Massive Dataset Loading âš¡\n",
        "\n",
        "This notebook demonstrates the 3-step process for using DataLFlash with massive datasets (Option A in the README).\n",
        "\n",
        "**The 3 Steps:**\n",
        "1. **Convert Dataset**: Convert your standard PyTorch dataset into optimized chunks.\n",
        "2. **Load Dataloaders**: Automatically load train/val/test loaders.\n",
        "3. **Train**: Use the loader just like a standard PyTorch DataLoader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Convert Dataset ðŸ“¦\n",
        "\n",
        "First, we'll create a dummy PyTorch dataset and then convert it into DataLFlash chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating dummy dataset...\n",
            "Converting to chunks...\n",
            "ðŸ“¦ Creando dataset chunked en: ./demo_data_chunks\n",
            "   ConfiguraciÃ³n: shuffle=False, splits={'train': 0.8, 'val': 0.1, 'test': 0.1}\n",
            "   RAM total: 47.82 GB\n",
            "   LÃ­mite de RAM: 33.47 GB\n",
            "ðŸ”„ Procesando 1000 muestras en chunks de 100...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Procesando dataset:  10%|â–ˆ         | 100/1000 [00:00<00:00, 926.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Chunk 0 guardado (100 muestras)\n",
            "ðŸ’¾ Chunk 1 guardado (100 muestras)\n",
            "ðŸ’¾ Chunk 2 guardado (100 muestras)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Procesando dataset:  30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [00:00<00:00, 1356.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Chunk 3 guardado (100 muestras)\n",
            "ðŸ’¾ Chunk 4 guardado (100 muestras)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Procesando dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [00:00<00:00, 1468.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Chunk 5 guardado (100 muestras)\n",
            "ðŸ’¾ Chunk 6 guardado (100 muestras)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Procesando dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [00:00<00:00, 1519.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Chunk 7 guardado (100 muestras)\n",
            "ðŸ’¾ Chunk 8 guardado (100 muestras)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Procesando dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1511.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Chunk 9 guardado (100 muestras)\n",
            "\n",
            "ðŸ“Š Resumen de la conversiÃ³n:\n",
            "   Muestras procesadas exitosamente: 1000\n",
            "   Muestras omitidas por error: 0\n",
            "   Total muestras en dataset original: 1000\n",
            "   Total chunks generados: 10\n",
            "âœ‚ï¸  DistribuciÃ³n de chunks:\n",
            "   Train: 8 chunks\n",
            "   Val:   1 chunks\n",
            "   Test:  1 chunks\n",
            "âœ… Metadata guardada en: ./demo_data_chunks\\metadata.json\n",
            "ðŸŽ‰ Dataset chunked creado con Ã©xito!\n",
            "   Directorio: ./demo_data_chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'./demo_data_chunks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from datalflash.utils import DatasetConverter\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. Create a dummy dataset (Simulating a massive dataset)\n",
        "print(\"Creating dummy dataset...\")\n",
        "X = torch.randn(1000, 3, 32, 32) # 1000 images of 32x32\n",
        "y = torch.randint(0, 10, (1000,)) # 1000 labels\n",
        "my_dataset = TensorDataset(X, y)\n",
        "\n",
        "# Define output directory\n",
        "output_dir = \"./demo_data_chunks\"\n",
        "\n",
        "# Clean up previous run if exists\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "# 2. Convert to DataLFlash Chunks\n",
        "print(\"Converting to chunks...\")\n",
        "DatasetConverter.create_chunked_dataset(\n",
        "    pytorch_dataset=my_dataset,\n",
        "    output_dir=output_dir,\n",
        "    chunk_size=100, # Small chunk size for this demo\n",
        "    split_ratios={'train': 0.8, 'val': 0.1, 'test': 0.1},\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Dataloaders ðŸš€\n",
        "\n",
        "Now we load the dataloaders from the chunked directory. DataLFlash handles the background loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataloaders...\n",
            "ðŸ“‚ Dataset cargado (Split: train)\n",
            "   Chunks: 8\n",
            "   Muestras: 800\n",
            "ðŸ’¾ Cargando chunk 0 (ID real: 0)...\n",
            "  Optimizando memoria del chunk...\n",
            "âœ… Chunk 0 cargado con 100 muestras\n",
            "ðŸ”® Precargando chunk 1 (ID real: 1) en background...\n",
            "ðŸ”„ Iniciando precarga de chunks en background...\n",
            "âœ… Loader 'train' creado: 800 muestras, 25 batches\n",
            "ðŸ”® Precargando chunk 2 (ID real: 2) en background...\n",
            "âœ… Loader 'val' creado: 100 muestras, 4 batches\n",
            "âœ… Loader 'test' creado: 100 muestras, 4 batches\n",
            "Train batches: 25\n",
            "Val batches: 4\n",
            "Test batches: 4\n"
          ]
        }
      ],
      "source": [
        "from datalflash.core import get_dataloaders\n",
        "\n",
        "print(\"Loading dataloaders...\")\n",
        "loaders = get_dataloaders(\n",
        "    chunks_dir=output_dir,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "train_loader = loaders['train']\n",
        "val_loader = loaders['val']\n",
        "test_loader = loaders['test']\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train ðŸ”¥\n",
        "\n",
        "Finally, we iterate through the loader just like a standard PyTorch DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training loop...\n",
            "Usando modo chunked optimizado (Samplers complejos ignorados en modo chunked puro)\n",
            "Batch 0: Features torch.Size([32, 3, 32, 32]), Targets torch.Size([32])\n",
            "âš¡ Usando chunk 1 precargado\n",
            "ðŸ”® Precargando chunk 3 (ID real: 3) en background...\n",
            "âš¡ Usando chunk 2 precargado\n",
            "ðŸ”® Precargando chunk 4 (ID real: 4) en background...\n",
            "âš¡ Usando chunk 3 precargado\n",
            "ðŸ”® Precargando chunk 5 (ID real: 5) en background...\n",
            "âš¡ Usando chunk 4 precargado\n",
            "ðŸ”® Precargando chunk 6 (ID real: 6) en background...\n",
            "âš¡ Usando chunk 5 precargado\n",
            "ðŸ”® Precargando chunk 7 (ID real: 7) en background...\n",
            "âš¡ Usando chunk 6 precargado\n",
            "âš¡ Usando chunk 7 precargado\n",
            "Training loop finished!\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting training loop...\")\n",
        "for i, (features, targets) in enumerate(train_loader):\n",
        "    # Your training logic here\n",
        "    if i == 0:\n",
        "        print(f\"Batch {i}: Features {features.shape}, Targets {targets.shape}\")\n",
        "    \n",
        "    # Simulate training\n",
        "    pass\n",
        "\n",
        "print(\"Training loop finished!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
